        * 7+ relevant years of work experience writing and maintaining distributed data pipelines.
        * Knowledge of Kafka and Zookeeper. Experience with writing Kafka consumers and/or producers.
        * Prior experience with AWS Redshift and/or PostgreSQL preferred.
        * Ability to translate the algorithms provided by data scientists and implement them in production.
        * Knowledge of Linux, network and file system, and database level troubleshooting.
        * Ability to manage, mentor, and grow a team
        * Experience in Python/Java.
        Our Tech Stack
        * AWS Redshift & Postgres - Data warehousing
        * Airflow - Data pipelines and ETL
        * AWS Database Migration Service (DMS) - for ETL
        * Scikit-learn for ML algorithms
        * Docker Stacks for sharing a common dev environment
        * Github, Docker Hub, CircleCI, Datadog, New Relic, Airbrake, Pager Duty
        We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.