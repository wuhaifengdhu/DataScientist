          About the Job
              Job Title         :: Senior ETL Lead/Developer – Hadoop Ecosystem
              Job Location  :: Houston, TX
              Job Duration ::   Full Time
               Job Description
               Responsibilities:
              ·         Architect, design, construct, test, tune, and deploy ETL infrastructure based on the Hadoop ecosystem based technologies.
              ·         Work closely with administrators, architects, and application teams to insure applications are performing well and within agreed upon SLAs.
              ·         Work closely with Management and Data Scientist teams to achieve company business objectives.
              ·         Collaborate with other technology teams and architects to define and develop solutions.
              ·         Lead/mentor developers in setting ETL architecture, design, and development standards.
              ·         Research and experiment with emerging ETL technologies and tools related to Big Data.
              ·         Contribute to the Big Data open source ecosystem.
              ·         Work with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed; ensuring a high degree of data quality.
              ·         Maintain, tune, and support the ETL platform on a day-to-day basis to insure high availability.
              ·         This is a hands-on role; you will lead by doing.
               Position Requirements
              :
              The Ideal Candidate will have:
              ·         Experience within the Life Insurance industry space preferred.
              ·         Excellent technical and organizational skills.
              ·         Strong communication and leadership skills.
              ·         Proficiencies with Agile development practices.
              ·         Experience with and strong understanding of Data Warehousing and Big Data Hadoop ecosystems.
              ·         Experience translating functional and technical requirements into technical specifications and design.
              ·         Knowledge and experience of ELT for Data Lake to ETL for the data servicing layer life cycle.
              ·         Experience with ELT/ETL batch, real-time, streaming, and messaging.
              ·         Experience with one or more of the following: Talend Big Data Integration Platform
              ·         Experience with Hadoop Map Reduce loading into a Data Warehouse.
              ·         Experience with Hive, Cassandra, DynamoDB, CouchDB a plus.
              ·         Experience with RDBMS technologies and SQL languages, Oracle & SQL Server a plus.
              Thanks & Regards,
              Naveen M
              HCL America Inc.
              Email:
                Mnaveen@hcl.com
          Report