         Description
         The role requires working closely with data science teams, frequently in a matrixed environment as part of a broader project team. As a senior-level position the role requires ‘self-starters’ who are proficient in problem solving and capable of bringing clarity to complex situations.  The culture of the organization places an emphasis on teamwork, so social and interpersonal skills are equally important as technical capability. Due to the emerging and fast-evolving nature of Big Data technology and practice, the position requires that one stay well-informed of technological advancements and be proficient at putting new innovations into effective practice.
          Responsibilities
         This role will provide application development for specific business environments. Focus on setting technical direction on groups of applications and similar technologies as well as taking responsibility for technically robust solutions encompassing all business, architecture, and technology constraints.
          Responsible for building and supporting a Hadoop-based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data.
          Manage and optimize Hadoop/Spark clusters, which may include many large HBase instances
          Support regular requests to move data from one cluster to another
          Manage production support teams to make sure service levels are maintained and any interruption is resolved in a timely fashion
          Bring new data sources into HDFS, transform and load to databases.
           Work collaboratively with Data Scientists and business and IT leaders throughout the company to understand Big Data needs and use cases.
         Qualifications:
         A successful candidate will have the following:
          Bachelor’s degree in Computer Science, or related discipline; with at least 5 years of equivalent work experience.
          Strong understanding of best practices and standards for Hadoop application design and implementation.
          Hands-on experience with Cloudera Distributed Hadoop (CDH) and experience with many of the following components:
            Hadoop, MapReduce, Spark, Impala, Hive, Solr, YARN
            Java, Python, or Scala
            SQL, JSON, XML
            RegEx
            Sqoop
          Experience in developing MapReduce programs using Apache Hadoop for working with Big Data.
          Experience having deployed Big Data Technologies to Production.
          Understanding of Lambda Design Architectures and Real-Time Streaming
          Ability to multitask and to balance competing priorities.
          Requires strong practical experience in agile application development, file systems management, and DevOps discipline and practice using short-cycle iterations to deliver continuous business value.
           Ability to define and utilize best practice techniques and to impose order in a fast-changing environment. Must have strong problem-solving skills.
          Strong verbal, written, and interpersonal skills, including a desire to work within a highly-matrixed, team-oriented environment.
          Preferred
         A successful candidate may have:
          Experience in Healthcare Domain
          Experience in Patient Data
          Hardware/Operating Systems:
          Linux
          UNIX
          Distributed, highly-scalable processing environments
          Networking - basic understanding of networking with respect to distributed server and file systems connectivity and troubleshooting of connectivity errors
          Databases
         :
           RDBMS – Teradata
           Other Languages – Java, Python, Scala, R
           Build Systems – Maven, Ant
           Source Control Systems – Git, Mercurial
           Continuous Integration Systems – Jenkins or Bamboo
           Config/Orchestration – Zookeeper, Puppet, Salt, Ansible, Chef, Oozie, Pig
          Certifications (a plus, but not required):
           CCDH (Cloudera Certified Developer for Apache Hadoop)