          About the Job
                AT&T offers a collaborative, fun, and fast paced environment.  If this is what drives you and you are ready for the next level in your career, then AT&T may have the perfect career opportunity for you!
                The Principal Big Data Software Engineer is responsible for the design & development of high performance, distributed computing tasks using Big Data technologies such as Hadoop, NoSQL, text mining and other distributed environment technologies. This position will use Big Data programming languages and technology, write code, complete programming and documentation, and perform testing and debugging of applications. It will also analyze, design, program, debug and modify software enhancements and/or new products used in distributed, large scale analytics and visualization solutions.  Position will interact with data scientists, business partners and industry experts to understand how data needs to be converted, loaded and presented.
                Specifically this Principal Big Data Software Engineer provides the backbone (the data and data platform) for the team as well as for the enterprise. This position helps to derive valuable insights and drive smart business decisions across AT&T.  Additional responsibilities include the following:
                 Works with the raw data, cleanses it and finally polishes it to the format where it can be consumed by Data Scientists to create critical insights.
                 Assists with ad-hoc requests coming from business partners. These can be simple statistics or complex operations.
                 Expert level competency with Scala, Java, Spark, Map/Reduce, high performance tuning, machine learning methods for classification and deep learning for pattern recognition.  In addition, the candidate must also have the ability to mentor and develop others in these technologies.
                 Strong communication and presentation skills are required to effectively convey relevant insights to the teams.
                 Performs analysis, implementation, and performance tuning for engineered artifacts.
                 Exercises judgment on how to effectively communicate highly technical and complex details through the use of visualization and careful selection of "knowns" versus "hypotheticals".
                Qualifications:
                 Masters of Science in Computer Science, Math or Scientific Computing preferred
                 Typically requires 8-10 years’ experience in Data architecture and data applications
                 Familiarity with JVM-based function languages including Scala and Clojure; Hadoop query languages including Pig, Hive, Scalding, Cascalog, PyCascading;
                 Familiarity with HDFS-based computing frameworks including Spark and STORM are desirable.
                 This position is based in Plano, TX. The candidate selected must work out of the local Plano office.
          Report